<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title> • foehnix</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">foehnix</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/foehnix.html">Getting started</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/foehnix.html">Foehnix Demo</a>
    </li>
    <li>
      <a href="../articles/simulation.html">Simulation Examples</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Advanced
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/statmodel.html">Statistical Model</a>
    </li>
    <li>
      <a href="../articles/advanced_simulation.html">Censoring and Truncation</a>
    </li>
    <li>
      <a href="../articles/families.html">foehnix Families</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/retostauffer/Rfoehnix">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1></h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/retostauffer/Rfoehnix/blob/master/../vignettes/statmodel.Rmd"><code>../vignettes/statmodel.Rmd</code></a></small>
      <div class="hidden name"><code>statmodel.Rmd</code></div>

    </div>

    
    
<div id="statistical-models" class="section level2">
<h2 class="hasAnchor">
<a href="#statistical-models" class="anchor"></a>Statistical Models</h2>
<p>The automated foehn classification <code>foehnix</code> is based on a two-component mixture model. The basic idea is that two unobservable components (or cluster) exist, one for situations without foehn, one for foehn situations. <code>foehnix</code> uses an unsupervised statistical model to identify these two components based on a set of observed values (e.g., wind speed, gust speed, potential temperature differences) to model the probability whether or not a specific observation is related to a foehn event or not.</p>
<p>The statistical model consists of two parts: one part identify the two components, and one part modelling the probability whether or not a specific observation belongs to component one or component two. The latter is known as the <em>concomitant model</em>.</p>
<p>The density of a two-component mixed distribution <span class="math inline">\(h(\dots)\)</span> in its general form is specified as follows:</p>
<ul>
<li><span class="math inline">\(h(\mathit{y}, \mathbf{x}, \mathit{\theta}, \mathit{\alpha}) =  \underbrace{(1 - \pi(\mathbf{x}, \alpha)) \cdot f(y, \mathit{\theta}_1)}_{\text{component 1}} +  \underbrace{\pi(\mathbf{x}, \alpha) \cdot f(\mathit{y}_i, \mathit{\theta}_2)}_{\text{component 2}}\)</span></li>
</ul>
<p>… where <span class="math inline">\(\mathit{y}\)</span> is the covariate for the first part identifying the two components, <span class="math inline">\(\mathbf{x}\)</span> are the covariates for the concomitant model. The density of the mixed distribution <span class="math inline">\(h\)</span> is the sum or superposition of the densities of the two components (<span class="math inline">\(f\)</span>; i.e., Gaussian distribution) times the probability <span class="math inline">\(\pi\)</span> whether or not a specific observation belongs to the second component. <span class="math inline">\(\mathit{\theta} = (\mathit{\theta}_1, \mathit{\theta}_2)\)</span> are the distribution parameters for the components, <span class="math inline">\(\mathit{\alpha}\)</span> the parameters for the concomitant model.</p>
<p>The concomitant model can be any model which fulfills <span class="math inline">\(\pi \in~]0,1[\)</span>, e.g., an constant value or intercept only model (mixture model <em>without concomitants</em>), or any kind of probability model. <code>foehnix</code> uses a logistic regression model of the following form:</p>
<ul>
<li><span class="math inline">\(\Big(\frac{\pi}{1 - \pi}\Big) = \mathbf{x}^\top \mathit{\alpha};~~  \pi = \frac{\exp(\mathbf{x}^\top \mathit{\alpha})}{1 + \exp(\mathbf{x}^\top \mathit{\alpha})}\)</span></li>
</ul>
<p>The final probability, also known as the a-posteriori probability, is then:</p>
<ul>
<li><span class="math inline">\(\hat{\mathit{p}}(\mathit{y}, \mathbf{x}, \mathit{\theta}, \mathit{\alpha}) =  \frac{\pi(\mathbf{x}, \mathit{alpha}) f(\mathbf{y}, \mathbf{\theta}_2)}{ (1 - \pi(\mathbf{x}, \mathit{alpha})) f(\mathbf{y}, \mathbf{\theta}_1) + \pi(\mathbf{x}, \mathit{alpha}) f(\mathbf{y}, \mathbf{\theta}_2)  }\)</span></li>
</ul>
<p>… where <span class="math inline">\(\hat{\mathit{p}}\)</span> in our case represents the probability of foehn. All one has to know are the parameters <span class="math inline">\(\mathit{\theta}\)</span> and <span class="math inline">\(\mathit{\alpha}\)</span> which can be estimated using an appropriate M-estimator such as maximum likelihood.</p>
</div>
<div id="parameter-estimation" class="section level2">
<h2 class="hasAnchor">
<a href="#parameter-estimation" class="anchor"></a>Parameter Estimation</h2>
<p>The maximum likelihood of a mixture model can usually not be maximized directly. One possibility to estimate model is an iterative <em>expectation maximization</em> (EM) algorithm to optimize the log-likelihood for a set of observation <span class="math inline">\(i = 1, \dots, N\)</span> given by:</p>
<ul>
<li>
<span class="math inline">\(\ell = \sum_{i=1}^N \big( (1 - \hat{\mathit{p}}) \cdot \log(f(\mathit{y}, \mathit{\theta}_1)) + \hat{\mathit{p}} \cdot \log(f(\mathit{y}, \mathit{\theta}_2)) + (1 - \hat{\mathit{p}} \cdot \log(1 - \mathit{\pi}(\mathbf{x}, \mathit{\alpha})) + \hat{\mathit{p}} \cdot \log(\mathit{\pi}(\mathbf{x}, \mathit{\alpha}))  \big)\)</span>.</li>
</ul>
<p>with <span class="math inline">\(\hat{\mathit{p}}\)</span> as specified above (a-posteriori probability).</p>
<p>The EM algorithm is as follows:</p>
<ul>
<li><p><strong>Initialization:</strong> initialize values for <span class="math inline">\(\mathbf{\theta}\)</span> and <span class="math inline">\(\mathit{\alpha}\)</span>.</p></li>
<li><p><strong>Estimation:</strong> compute the posterior class probability <span class="math inline">\(\hat{\mathit{p}}(\mathit{y}, \mathbf{x}, \mathit{\theta}, \mathit{\alpha})\)</span></p></li>
<li><p><strong>Maximize:</strong> estimate <span class="math inline">\(\mathit{\theta}\)</span> and <span class="math inline">\(\mathit{\alpha}\)</span> which maximize the likelihood using the posterior class probability <span class="math inline">\(\hat{\mathit{p}}\)</span> from the estimation step as weights:</p></li>
<li><p><span class="math inline">\(\hat{\mathit{p}}(\mathit{y}, \mathbf{x}, \mathit{\theta}, \mathit{\alpha}) = \frac{\hat{p}(\mathbf{x}, \mathit{alpha}) f(\mathbf{y}, \mathbf{\theta}_2)}{  (1 - \hat{p}(\mathbf{x}, \mathit{alpha})) f(\mathbf{y}, \mathbf{\theta}_1) +  \hat{p}(\mathbf{x}, \mathit{alpha}) f(\mathbf{y}, \mathbf{\theta}_2) }\)</span></p></li>
</ul>
<p>The EM steps are repeated until the likelihood improvement falls below a certain limit or the maximum number of iterations is reached.</p>
</div>
<div id="gaussian-mixture-model-without-concomitants" class="section level2">
<h2 class="hasAnchor">
<a href="#gaussian-mixture-model-without-concomitants" class="anchor"></a>Gaussian Mixture Model Without Concomitants</h2>
<p>The simplest case is a Gaussian two-component mixture model without concomitants. In this case the density of the two components is the density <span class="math inline">\(\phi\)</span> of the Gaussian distribution with its parameters <span class="math inline">\(\mathit{\theta}_1 = (\mu_1, \sigma_1)\)</span> and <span class="math inline">\(\mathit{\theta}_2 = (\mu_2, \sigma_2)\)</span> where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the <em>location</em> and <em>scale parameter</em> of the Gaussian distribution, or <em>mean</em> and <em>standard deviation</em>.</p>
<div id="initialization-step" class="section level4">
<h4 class="hasAnchor">
<a href="#initialization-step" class="anchor"></a>Initialization step</h4>
<p>First, initial values for the parameters (<span class="math inline">\(\mathit{\theta}\)</span>) and the posterior weights (<span class="math inline">\(\hat{\mathit{p}}\)</span>) have to be specified. <span class="math inline">\(\mathit{\alpha}\)</span> does not have to be initialized as no concomitant model is used in this case! To be able to do so we have to attribute each observation <span class="math inline">\(\mathit{y}_i \forall i = 1, \dots, N\)</span> to one of the two components. This initial membership will be denoted as <span class="math inline">\(\mathit{z}\)</span> and takes 1 if observation <span class="math inline">\(y_i\)</span> belongs to <em>component 2</em> and 0 else. This initial attribution defines that observations with high values of <span class="math inline">\(\mathit{y}\)</span> belong to component 2, observations with low values of <span class="math inline">\(\mathit{y}\)</span> to component 1.</p>
<p><strong>Note:</strong> Depending on the model specification this can lead to models where the probability for <em>no foehn</em> will be returned by <code>foehnix</code> rather than posteriori probability of <em>foehn</em>. However, the <code>switch</code> argument of the <code><a href="../reference/foehnix.html">foehnix(...)</a></code> function allows you to control this behavior (see <a href="reference/foehnix.html"><code>foehnix</code> manual page</a>).</p>
<p><code>foehnix</code> uses the following initialization for the two-component Gaussian mixture model without concomitants:</p>
<ol style="list-style-type: decimal">
<li>Initialize class membership: <span class="math inline">\(z_i = \begin{cases}1 &amp; \text{if}~y_i \ge \bar{y} \\ 0 &amp; \text{else}\end{cases}\)</span>
</li>
<li>Initial parameters for <span class="math inline">\(\mathbf{\theta}^{(0)}\)</span> using weighted empirical moments for <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\mu_2\)</span> and the standard deviation of <span class="math inline">\(y\)</span> as initial guess for <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>:
<ul>
<li><span class="math inline">\(\mu_1^{(0)} = \frac{1}{\sum_{i=1}^{N} (1-z_i)} \sum_{i=1}^{N} (1-z_i) \cdot y_i\)</span></li>
<li><span class="math inline">\(\mu_2^{(0)} = \frac{1}{\sum_{i=1}^{N} z_i} \sum_{i=1}^{N} z_i \cdot y_i\)</span></li>
<li><span class="math inline">\(\sigma_1^{(0)} = \sigma_2^{(0)} = \big(\frac{1}{N} \sum_{i=1}^{N} (y_i - \bar{y})^2\big)^\frac{1}{2}\)</span></li>
</ul>
</li>
<li>Initialize <span class="math inline">\(\mathit{\pi}^{(0)} = 0.5\)</span>
</li>
<li>Given <span class="math inline">\(\mathit{\theta}^{(0)}\)</span> and <span class="math inline">\(\mathit{\pi}^{(0)}\)</span>: calculate a-posteriory probability:
<ul>
<li><span class="math inline">\(\hat{\mathit{p}}^{(0)} = \frac{\mathit{\pi}^{(0)} \cdot \phi(\mathit{y}, \mathit{\theta}_2^{(0)})}{  (1 - \mathit{\pi}^{(0)}) \cdot \phi(\mathit{y}, \mathit{\theta}_1^{(0)})  + \mathit{\pi}^{(0)} \cdot \phi(\mathit{y}, \mathit{\theta}_2^{(0)})}\)</span></li>
</ul>
</li>
</ol>
<p>Once the required elements have been initialized start the EM algorithm for <span class="math inline">\(j = 1, ..., maxit\)</span>:</p>
<ol start="5" style="list-style-type: decimal">
<li>Update <span class="math inline">\(\pi^{(j)} = \text{mean}(\hat{\mathit{p}}^{(j-1)})\)</span>
</li>
<li>Obtain new <span class="math inline">\(\mathit{\theta}^{(j)}\)</span> using <span class="math inline">\(\hat{\mathit{p}}^{(j-1)}\)</span>:
<ul>
<li><span class="math inline">\(\mu_1^{(j)} = \frac{1}{\sum_{i=1}^{N} (1 - \hat{p}_i^{(j-1)}})  \sum_{i=1}^{N} (1 - \hat{\mathit{p}}_i^{(j-1)}) \cdot y_i\)</span></li>
<li><span class="math inline">\(\mu_2^{(j)} = \frac{1}{\sum_{i=1}^{N} \hat{p}_i^{(j-1)}}  \sum_{i=1}^{N} \hat{\mathit{p}}_i^{(j-1)} \cdot y_i\)</span></li>
<li><span class="math inline">\(\sigma_1^{(j)} = \Big(\frac{1}{\sum_{i=1}^{N} (1-\hat{p}_i^{(j-1)})}  \sum_{i=1}^{N} (1 - \hat{p}_i^{(j-1)}) \cdot (y_i - \bar{y})^2\Big)^\frac{1}{2}\)</span></li>
<li><span class="math inline">\(\sigma_2^{(j)} = \Big(\frac{1}{\sum_{i=1}^{N} \hat{p}_i^{(j-1)}}  \sum_{i=1}^{N} \hat{p}_i^{(j-1)} \cdot (y_i - \bar{y})^2\Big)^\frac{1}{2}\)</span></li>
</ul>
</li>
<li>Update posterior probabilities <span class="math inline">\(\hat{\mathit{p}}^{(j)}\)</span>:
<ul>
<li><span class="math inline">\(\hat{\mathit{p}}^{(j)} = \frac{\mathit{\pi}^{(j)} \cdot \phi(\mathit{y}, \mathit{\theta}_2^{(j)})}{  (1 - \mathit{\pi}^{(j)}) \cdot \phi(\mathit{y}, \mathit{\theta}_1^{(j)})  + \mathit{\pi}^{(j)} \cdot \phi(\mathit{y}, \mathit{\theta}_2^{(j)})}\)</span></li>
</ul>
</li>
<li>Calculate likelihood: <span class="math inline">\(\ell^{(j)}\)</span>. If <span class="math inline">\(j = 1\)</span> proceed with <strong>step 5</strong>.</li>
<li>For <span class="math inline">\(j &gt; 1\)</span>: if <span class="math inline">\((\ell^{(j)} - \ell^{(j-1)}) &lt; \text{tol}\)</span> the likelihood could not have been improved (converged or stuck): stop EM algorithm. If <span class="math inline">\(j = \text{maxit}\)</span>: maximum number of iterations reached, stop EM algorithm. Else proceed with <strong>step 5</strong> until one of the stopping criteria is reached.</li>
</ol>
</div>
</div>
<div id="gaussian-mixture-model-with-concomitants" class="section level2">
<h2 class="hasAnchor">
<a href="#gaussian-mixture-model-with-concomitants" class="anchor"></a>Gaussian Mixture Model With Concomitants</h2>
<p>The optimizer for a two-component Gaussian mixture model with additional concomitants is very similar except that we also have to update the concomitant model (logistic regression model). For mixed models with concomitants the probabilities <span class="math inline">\(\mathit{\pi}\)</span> are a function of the concomitant covariates <span class="math inline">\(\mathbf{x}\)</span> and the regression coefficients <span class="math inline">\(\mathit{\alpha}\)</span>.</p>
<p>The following algorithm is used:</p>
<ol style="list-style-type: decimal">
<li>Initialize class membership <span class="math inline">\(\mathit{z}\)</span> <em>as for the Gaussian mixture model without concomitants</em>.</li>
<li>Initialize coefficients <span class="math inline">\(\mathit{\theta}^{(0)}\)</span> <em>as for the Gaussian mixture model without concomitants</em>.</li>
<li>Given <span class="math inline">\(\mathit{z}^{(0)}\)</span> and <span class="math inline">\(\mathbf{x}\)</span>: estimate logistic regression model to obtain the parameters <span class="math inline">\(\mathit{\alpha}^{(0)}\)</span>, calculate <span class="math inline">\(\mathit{\pi}^{(0)} = \frac{\exp(\mathbf{x}^\top \mathit{\alpha}}{1 + \exp(\mathbf{x}^\top \mathit{\alpha}}\)</span> (see <a href="reference/iwls_logit.html">iwls_logit</a>).</li>
<li>Calculate a-posteriori probability <span class="math inline">\(\hat{\mathit{p}}^{(0)}\)</span> <em>as for the Gaussian mixture model without concomitants</em>.</li>
</ol>
<p>The EM algorithm for <span class="math inline">\(j = 1, \dots, \text{maxit}\)</span>:</p>
<ol start="5" style="list-style-type: decimal">
<li>Update <span class="math inline">\(\pi^{(j)}\)</span> by updating the concomitant model (logistic regression model) using <span class="math inline">\(\hat{\mathit{p}}^{(j-1)}\)</span> as response for the concomitant model (see <a href="reference/iwls_logit.html">iwls_logit</a>).</li>
<li>Obtain new <span class="math inline">\(\mathit{\theta}^{(j)}\)</span> <em>as for the Gaussian mixture model without concomitants</em>.</li>
<li>Update posterior probabilities <span class="math inline">\(\hat{\mathit{p}}^{(j)}\)</span> <em>as for the Gaussian mixture model without concomitants</em>.</li>
<li>Calculate likelihood <em>as for the Gaussian mixture model without concomitants</em>.</li>
<li>
<em>As for the Gaussian mixture model without concomitants</em>: proceed with <strong>step 5</strong> until one of the stopping criteria is reached.</li>
</ol>
</div>
<div id="logistic-regression-model" class="section level2">
<h2 class="hasAnchor">
<a href="#logistic-regression-model" class="anchor"></a>Logistic Regression Model</h2>
<p>The logistic two-component mixture models can be estimated as the Gaussian ones except that component density is the density of the logistic distribution, and that the weighted empirical moments for <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>, the scale of the logistic distribution, is now:</p>
<ul>
<li><span class="math inline">\(\sigma_1^{(j)} = \Big(\frac{1}{\sum_{i=1}^{N} (1-\hat{p}_i^{(j-1)})}  \sum_{i=1}^{N} (1 - \hat{p}_i^{(j-1)}) \cdot (y_i - \bar{y})^2\Big)^\frac{1}{2}  \cdot \frac{\sqrt{3}}{3.1415}\)</span></li>
<li><span class="math inline">\(\sigma_2^{(j)} = \Big(\frac{1}{\sum_{i=1}^{N} \hat{p}_i^{(j-1)}}  \sum_{i=1}^{N} \hat{p}_i^{(j-1)} \cdot (y_i - \bar{y})^2\Big)^\frac{1}{2}  \cdot \frac{\sqrt{3}}{3.1415}\)</span></li>
</ul>
</div>
<div id="censored-and-truncated-models" class="section level2">
<h2 class="hasAnchor">
<a href="#censored-and-truncated-models" class="anchor"></a>Censored and Truncated Models</h2>
<p>In case of a censored or truncated mixed model the distributional parameters <span class="math inline">\(\mathit{\theta}\)</span> of the components of the mixture model cannot be calculated using weighted empirical moments. In these cases a numreical likelihood-based solver is used to estimate <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\mu_2\)</span>, <span class="math inline">\(\sigma_1\)</span>, and <span class="math inline">\(\sigma_2\)</span>.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#statistical-models">Statistical Models</a></li>
      <li><a href="#parameter-estimation">Parameter Estimation</a></li>
      <li><a href="#gaussian-mixture-model-without-concomitants">Gaussian Mixture Model Without Concomitants</a></li>
      <li><a href="#gaussian-mixture-model-with-concomitants">Gaussian Mixture Model With Concomitants</a></li>
      <li><a href="#logistic-regression-model">Logistic Regression Model</a></li>
      <li><a href="#censored-and-truncated-models">Censored and Truncated Models</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Reto Stauffer.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.9000.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
